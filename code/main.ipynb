{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel in New York City during the Christmas Holidays: An Analysis of Destinations, Trends, and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集来源及获取\n",
    "本文主要用到了如下数据集：\n",
    "1. 纽约市2017年-2021每年12月的黄色出租车和绿色出租车数据 - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "2. 对应数据区域的shapefile - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "3. 对应时间的天气数据（API获取） - [OpenWeather](https://openweathermap.org/api/one-call-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 天气数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入获取天气数据代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出租车行程数据处理\n",
    "这一节旨在将出租车形成数据集计为小时车流数据，集计后的数据包括：\n",
    "1. 日期和小时\n",
    "2. 出发区域ID\n",
    "3. 到达区域ID\n",
    "4. 行程数量\n",
    "5. 行程人数\n",
    "6. 旅行费用\n",
    "7. 载客系数（行程人数/行程数量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入数据集计代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the hot points\n",
    "* calculate the in-and-out volumn for each zone by hours\n",
    "* give the zones a rank, transfer the rank to point\n",
    "* add all time points, find the hotest points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the score of each point per hour.   \n",
    "Because of the time complexity of bellow function is too high, I used AWS to calculate the traffic in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score for each zone by day and hours\n",
    "def get_in_out_volumn_score(df, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn.\n",
    "    Input:\n",
    "        df: a dataframe containing the counted data\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "    \"\"\"\n",
    "    # calculate the in-and-out volumn for each zone by day and hours\n",
    "    ## get the location id\n",
    "    location_id_list = list(range(1,264))\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## get the in-and-out volumn for each zone by day and hours, and store them in a dictionary\n",
    "    in_out_volumn = {}\n",
    "    for location_id in location_id_list:\n",
    "        for day in day_list:\n",
    "            for hour in hour_list:\n",
    "                in_out_volumn[(location_id, day, hour)] = [df.loc[(df['DOLocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum(), df.loc[(df['PULocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum()]\n",
    "        print(f'Finish calculating in-and-out volumn for zone {location_id} of {year}.')\n",
    "    # convert the dictionary to a dataframe\n",
    "    df_in_out_volumn = pd.DataFrame.from_dict(in_out_volumn, orient='index', columns=['in_volumn', 'out_volumn'])\n",
    "    # add day hour and location id as columns\n",
    "    df_in_out_volumn['day'] = df_in_out_volumn.index.map(lambda x: x[1])\n",
    "    df_in_out_volumn['hour'] = df_in_out_volumn.index.map(lambda x: x[2])\n",
    "    df_in_out_volumn['location_id'] = df_in_out_volumn.index.map(lambda x: x[0])\n",
    "    # add the total volumn\n",
    "    df_in_out_volumn['total_volumn'] = df_in_out_volumn['in_volumn'] + df_in_out_volumn['out_volumn']\n",
    "    # reorder the columns\n",
    "    df_in_out_volumn = df_in_out_volumn[['day', 'hour', 'location_id', 'in_volumn', 'out_volumn', 'total_volumn']]\n",
    "    # reset the index\n",
    "    df_in_out_volumn = df_in_out_volumn.reset_index(drop=True)\n",
    "\n",
    "    # calculate the rank of the total volumn for each zone by day and hours\n",
    "    df_in_out_volumn['total_volumn_rank'] = df_in_out_volumn.groupby(['day', 'hour'])['total_volumn'].rank(ascending=False)\n",
    "    # give the zone a score = 1/rank * number of zones\n",
    "    df_in_out_volumn['total_volumn_score'] = (1/df_in_out_volumn['total_volumn_rank']) * 263\n",
    "    \n",
    "    return df_in_out_volumn\n",
    "\n",
    "def plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Plot the score for each zone by day and hours.\n",
    "    Input:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # plot the score for each zone by day and hours\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## plot the score for each zone by day and hours\n",
    "    for day in day_list:\n",
    "        for hour in hour_list:\n",
    "            plt.figure(figsize=(20,10))\n",
    "            sns.barplot(x='location_id', y='total_volumn_score', data=df_in_out_volumn.loc[(df_in_out_volumn['day']==day) & (df_in_out_volumn['hour']==hour), :])\n",
    "            plt.title('day: {}, hour: {}'.format(day, hour))\n",
    "            plt.xticks([])\n",
    "            plt.savefig(f'../data/figures/in_out_volumn_score{year}_{day}_{hour}.png')\n",
    "            plt.close()\n",
    "\n",
    "def get_hot_score(year_list, day_list, hour_list):\n",
    "    for year in year_list:\n",
    "        # read the data\n",
    "        df = pd.read_csv(f'../data/processed_nyc_data/countdata_{year}-12.csv')\n",
    "        # calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn\n",
    "        df_in_out_volumn = get_in_out_volumn_score(df, day_list, hour_list,year)\n",
    "        df_in_out_volumn.to_csv(f'../data/processed_nyc_data/in_out_volumn_score_{year}-12.csv', index=False)\n",
    "        print(f'Finish getting score for {year}-12.')\n",
    "        # plot the score for each zone by day and hours\n",
    "        plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list = range(2017, 2022)  # 2017-2021\n",
    "day_list = range(1, 32)  # 1-31\n",
    "hour_list = range(0, 24)  # 0-23\n",
    "get_hot_score(year_list, day_list, hour_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the hotest points for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_16_hotest_zone(year):\n",
    "    df = pd.read_csv(f'../data/processed_nyc_data/{year}.csv')\n",
    "    df['year_score'] = df.groupby('location_id')['total_volumn_score'].transform('sum')\n",
    "    df['year_volumn'] = df.groupby('location_id')['total_volumn'].transform('sum')\n",
    "    df_hot = df[['location_id', 'year_score','year_volumn']].drop_duplicates()\n",
    "    df_hot['volumn_rank'] = df_hot['year_volumn'].rank(ascending=False)\n",
    "    df_hot['score_rank'] = df_hot['year_score'].rank(ascending=False)\n",
    "    df_hot = df_hot.sort_values(by='year_score', ascending=False)\n",
    "    df_hot = df_hot.head(16)\n",
    "    df_hot = df_hot[['location_id', 'year_score', 'score_rank', 'year_volumn', 'volumn_rank']]\n",
    "\n",
    "    return df_hot\n",
    "\n",
    "def plot_hot_zone(df_hot, df_geo, year):\n",
    "    # merge the year score to the geodata\n",
    "    df_hot_geo = df_geo.merge(df_hot, left_on='LocationID', right_on='location_id', how='right')\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_hot_geo.plot(column='year_score', cmap='YlOrRd', edgecolor='black', legend=True)\n",
    "    plt.savefig(f'../data/figures/hot_zone_{year}.png')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def get_hot_zone(year_list):\n",
    "    df_hots = {}\n",
    "    df_geo = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "    for year in year_list:\n",
    "        # get the 16 hotest zone for each year\n",
    "        df_hot = get_16_hotest_zone(year)\n",
    "        # plot the hot zone\n",
    "        plot_hot_zone(df_hot, df_geo, year)\n",
    "        # add the df_hot to a dictionary\n",
    "        df_hots[year] = df_hot\n",
    "\n",
    "    return df_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "hot_zones = get_hot_zone(range(2017, 2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get year total_volumn for each zone\n",
    "def get_year_total_volumn(df, year):\n",
    "    df['year'] = year\n",
    "    df['total_volumn'] = df['in_volumn'] + df['out_volumn']\n",
    "    df_year = df[['year', 'location_id', 'total_volumn']]\n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_zones[2021]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经做了的：\n",
    "1. 给所有区域以年为单位进行流量和热度排名,其中：\n",
    "   1. 流量排名：以31天24小时流量和进行排名，这反映了其在这一个月总的流量\n",
    "   2. 热度排名：以31天24小时热度分数和进行排名，这反映了其在一个月中成为热点区域的频率\n",
    "2. 找到每年热点区域（用于后续时间序列预测）\n",
    "3. 流量排名和热度排名的差距显示了一些地区车流量的波动性显著强于另一些地区\n",
    "\n",
    "要做的：\n",
    "1. 比较各热点区域历年排名变化（可视化）\n",
    "2. 比较一天内热点变化（工作日，一般周末，圣诞假期）（可视化）\n",
    "3. auto_arima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = df_geo.to_crs(epsg=4326)\n",
    "df_geo[df_geo['LocationID']==138].centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间定性分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 比较历年热点区域变化情况\n",
    "2. 比较一天内热点变化趋势（工作日、周末、圣诞节）\n",
    "3. 比较同一区域的流量排名和热度排名，分类如下：\n",
    "   1. 流量排名/热度排名 > 1 ：时间波动性较强\n",
    "   2. 流量排名/热度排名 < 1 : 时间波动性较弱\n",
    "4. 分别可视化波动性较强和较弱的区域，观察其分布\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 确定用于ARIMA分析的10个区域\n",
    "2. 生成这些区域间交通流量的时间序列（5\\*10\\*10）（包括天气信息）\n",
    "3. 对500条时间序列进行ARIMA分解，获取其总体趋势和季节性趋势\n",
    "4. 总结结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTUA_Ass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73aa197a937c6d4ce912cf3eb748cc19d4a43b8337a43d316b8eed91cf78bf92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
