{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel in New York City during the Christmas Holidays: An Analysis of Destinations, Trends, and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pmdarima import auto_arima\n",
    "from IPython.display import HTML\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集来源及获取\n",
    "本文主要用到了如下数据集：\n",
    "1. 纽约市2017年-2021每年12月的黄色出租车和绿色出租车数据 - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "2. 对应数据区域的shapefile - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "3. 对应时间的天气数据（API获取） - [OpenWeather](https://openweathermap.org/api/one-call-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 天气数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入获取天气数据代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入区域地理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zones_geo = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "#transform the coordinate system to WGS84\n",
    "nyc_zones_geo = nyc_zones_geo.to_crs(epsg=4326)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出租车行程数据处理\n",
    "这一节旨在将出租车形成数据集计为小时车流数据，集计后的数据包括：\n",
    "1. 日期和小时\n",
    "2. 出发区域ID\n",
    "3. 到达区域ID\n",
    "4. 行程数量\n",
    "5. 行程人数\n",
    "6. 旅行费用\n",
    "7. 载客系数（行程人数/行程数量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入数据集计代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the hot points\n",
    "* calculate the in-and-out volumn for each zone by hours\n",
    "* give the zones a rank, transfer the rank to point\n",
    "* add all time points, find the hotest points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the score of each point per hour.   \n",
    "Because of the time complexity of bellow function is too high, I used AWS to calculate the traffic in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score for each zone by day and hours\n",
    "def get_in_out_volumn_score(df, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn.\n",
    "    Input:\n",
    "        df: a dataframe containing the counted data\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "    \"\"\"\n",
    "    # calculate the in-and-out volumn for each zone by day and hours\n",
    "    ## get the location id\n",
    "    location_id_list = list(range(1,264))\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## get the in-and-out volumn for each zone by day and hours, and store them in a dictionary\n",
    "    in_out_volumn = {}\n",
    "    for location_id in location_id_list:\n",
    "        for day in day_list:\n",
    "            for hour in hour_list:\n",
    "                in_out_volumn[(location_id, day, hour)] = [df.loc[(df['DOLocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum(), df.loc[(df['PULocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum()]\n",
    "        print(f'Finish calculating in-and-out volumn for zone {location_id} of {year}.')\n",
    "    # convert the dictionary to a dataframe\n",
    "    df_in_out_volumn = pd.DataFrame.from_dict(in_out_volumn, orient='index', columns=['in_volumn', 'out_volumn'])\n",
    "    # add day hour and location id as columns\n",
    "    df_in_out_volumn['day'] = df_in_out_volumn.index.map(lambda x: x[1])\n",
    "    df_in_out_volumn['hour'] = df_in_out_volumn.index.map(lambda x: x[2])\n",
    "    df_in_out_volumn['location_id'] = df_in_out_volumn.index.map(lambda x: x[0])\n",
    "    # add the total volumn\n",
    "    df_in_out_volumn['total_volumn'] = df_in_out_volumn['in_volumn'] + df_in_out_volumn['out_volumn']\n",
    "    # reorder the columns\n",
    "    df_in_out_volumn = df_in_out_volumn[['day', 'hour', 'location_id', 'in_volumn', 'out_volumn', 'total_volumn']]\n",
    "    # reset the index\n",
    "    df_in_out_volumn = df_in_out_volumn.reset_index(drop=True)\n",
    "\n",
    "    # calculate the rank of the total volumn for each zone by day and hours\n",
    "    df_in_out_volumn['total_volumn_rank'] = df_in_out_volumn.groupby(['day', 'hour'])['total_volumn'].rank(ascending=False)\n",
    "    # give the zone a score = 1/rank * number of zones\n",
    "    df_in_out_volumn['total_volumn_score'] = (1/df_in_out_volumn['total_volumn_rank']) * 263\n",
    "    \n",
    "    return df_in_out_volumn\n",
    "\n",
    "def plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Plot the score for each zone by day and hours.\n",
    "    Input:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # plot the score for each zone by day and hours\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## plot the score for each zone by day and hours\n",
    "    for day in day_list:\n",
    "        for hour in hour_list:\n",
    "            plt.figure(figsize=(20,10))\n",
    "            sns.barplot(x='location_id', y='total_volumn_score', data=df_in_out_volumn.loc[(df_in_out_volumn['day']==day) & (df_in_out_volumn['hour']==hour), :])\n",
    "            plt.title('day: {}, hour: {}'.format(day, hour))\n",
    "            plt.xticks([])\n",
    "            plt.savefig(f'../data/figures/in_out_volumn_score{year}_{day}_{hour}.png')\n",
    "            plt.close()\n",
    "\n",
    "def get_hot_score(year_list, day_list, hour_list):\n",
    "    for year in year_list:\n",
    "        # read the data\n",
    "        df = pd.read_csv(f'../data/processed_nyc_data/countdata_{year}-12.csv')\n",
    "        # calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn\n",
    "        df_in_out_volumn = get_in_out_volumn_score(df, day_list, hour_list,year)\n",
    "        df_in_out_volumn.to_csv(f'../data/processed_nyc_data/in_out_volumn_score_{year}-12.csv', index=False)\n",
    "        print(f'Finish getting score for {year}-12.')\n",
    "        # plot the score for each zone by day and hours\n",
    "        plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These codes run in AWS\n",
    "# year_list = range(2017, 2022)  # 2017-2021\n",
    "# day_list = range(1, 32)  # 1-31\n",
    "# hour_list = range(0, 24)  # 0-23\n",
    "# get_hot_score(year_list, day_list, hour_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the hotest points for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(year):\n",
    "    df = pd.read_csv(f'../data/processed_nyc_data/{year}.csv')\n",
    "    df['year_score'] = df.groupby('location_id')['total_volumn_score'].transform('sum')\n",
    "    df['year_volumn'] = df.groupby('location_id')['total_volumn'].transform('sum')\n",
    "    df_hot = df[['location_id', 'year_score','year_volumn']].drop_duplicates()\n",
    "\n",
    "    df_hot['volumn_rank'] = df_hot['year_volumn'].rank(ascending=False)\n",
    "    df_hot['score_rank'] = df_hot['year_score'].rank(ascending=False)\n",
    "    \n",
    "    df_hot['fluctuation_coef'] = df_hot['score_rank'] / df_hot['volumn_rank']\n",
    "    df_hot['fluctuation_rank'] = df_hot['fluctuation_coef'].rank(ascending=False)\n",
    "    \n",
    "    df_hot = df_hot.sort_values(by='year_score', ascending=False)\n",
    "    df_hot = df_hot[['location_id', 'year_score', 'score_rank', 'year_volumn', 'volumn_rank', 'fluctuation_coef', 'fluctuation_rank']]\n",
    "\n",
    "    return df_hot\n",
    "\n",
    "def plot_hot_zone(df_hot, df_geo, year):\n",
    "    # merge the year score to the geodata\n",
    "    df_hot_geo = df_geo.merge(df_hot, left_on='LocationID', right_on='location_id', how='right')\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_hot_geo.plot(column='year_score', cmap='YlOrRd', edgecolor='black', legend=True)\n",
    "    plt.savefig(f'../data/figures/hot_zone_{year}.png')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def get_zones_rank(year_list):\n",
    "    df_hots = {}\n",
    "    for year in year_list:\n",
    "        # get the 16 hotest zone for each year\n",
    "        df_hot = get_ranks(year)\n",
    "        df_hots[year] = df_hot\n",
    "\n",
    "    return df_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>year_score</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>year_volumn</th>\n",
       "      <th>volumn_rank</th>\n",
       "      <th>fluctuation_coef</th>\n",
       "      <th>fluctuation_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174840</th>\n",
       "      <td>236</td>\n",
       "      <td>77179.852388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>681674.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175584</th>\n",
       "      <td>237</td>\n",
       "      <td>74142.578174</td>\n",
       "      <td>2.0</td>\n",
       "      <td>675819.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119040</th>\n",
       "      <td>161</td>\n",
       "      <td>58747.474519</td>\n",
       "      <td>3.0</td>\n",
       "      <td>592223.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>131.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34968</th>\n",
       "      <td>48</td>\n",
       "      <td>58554.598364</td>\n",
       "      <td>4.0</td>\n",
       "      <td>496434.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58032</th>\n",
       "      <td>79</td>\n",
       "      <td>46685.331204</td>\n",
       "      <td>5.0</td>\n",
       "      <td>406746.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>263.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        location_id    year_score  score_rank  year_volumn  volumn_rank  \\\n",
       "174840          236  77179.852388         1.0     681674.0          1.0   \n",
       "175584          237  74142.578174         2.0     675819.0          2.0   \n",
       "119040          161  58747.474519         3.0     592223.0          3.0   \n",
       "34968            48  58554.598364         4.0     496434.0          7.0   \n",
       "58032            79  46685.331204         5.0     406746.0         13.0   \n",
       "\n",
       "        fluctuation_coef  fluctuation_rank  \n",
       "174840          1.000000             131.0  \n",
       "175584          1.000000             131.0  \n",
       "119040          1.000000             131.0  \n",
       "34968           0.571429             261.0  \n",
       "58032           0.384615             263.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_zones = get_zones_rank(range(2017, 2022))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>year_score</th>\n",
       "      <th>score_rank</th>\n",
       "      <th>year_volumn</th>\n",
       "      <th>volumn_rank</th>\n",
       "      <th>fluctuation_coef</th>\n",
       "      <th>fluctuation_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>175584</th>\n",
       "      <td>237</td>\n",
       "      <td>82742.029103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>293589.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174840</th>\n",
       "      <td>236</td>\n",
       "      <td>76948.211696</td>\n",
       "      <td>2.0</td>\n",
       "      <td>288884.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34968</th>\n",
       "      <td>48</td>\n",
       "      <td>61368.859444</td>\n",
       "      <td>3.0</td>\n",
       "      <td>194108.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97464</th>\n",
       "      <td>132</td>\n",
       "      <td>50773.052077</td>\n",
       "      <td>4.0</td>\n",
       "      <td>162677.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119040</th>\n",
       "      <td>161</td>\n",
       "      <td>47698.763971</td>\n",
       "      <td>5.0</td>\n",
       "      <td>241028.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        location_id    year_score  score_rank  year_volumn  volumn_rank  \\\n",
       "175584          237  82742.029103         1.0     293589.0          1.0   \n",
       "174840          236  76948.211696         2.0     288884.0          2.0   \n",
       "34968            48  61368.859444         3.0     194108.0          6.0   \n",
       "97464           132  50773.052077         4.0     162677.0         13.0   \n",
       "119040          161  47698.763971         5.0     241028.0          3.0   \n",
       "\n",
       "        fluctuation_coef  fluctuation_rank  \n",
       "175584          1.000000             133.0  \n",
       "174840          1.000000             133.0  \n",
       "34968           0.500000             261.0  \n",
       "97464           0.307692             263.0  \n",
       "119040          1.666667               2.0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_zones[2021].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 30 hot zone's fluctuation rank on the map\n",
    "def plot_hot_zone(dict_hot, df_geo, year):\n",
    "    # merge the year score to the geodata\n",
    "    df_hot_geo = df_geo.merge(dict_hot[year], left_on='LocationID', right_on='location_id', how='right')\n",
    "\n",
    "    # filter the hot zone which volumn_rank is less than 31\n",
    "    df_hot_geo = df_hot_geo.loc[df_hot_geo['volumn_rank']<=30, :]\n",
    "\n",
    "    # add center point coordinates to the dataframe\n",
    "    df_hot_geo['center_lat'] = df_hot_geo['geometry'].centroid.y\n",
    "    df_hot_geo['center_lon'] = df_hot_geo['geometry'].centroid.x\n",
    "\n",
    "\n",
    "    threshold_scale = np.linspace(df_hot_geo['fluctuation_rank'].min(),\n",
    "                              df_hot_geo['fluctuation_rank'].max(), 4, dtype=float).tolist()\n",
    "\n",
    "    \n",
    "    # plot the hot zone with base map in folium\n",
    "    m = folium.Map(location=[40.75, -73.9], zoom_start=11, tiles='Stamen Toner')\n",
    "    folium.Choropleth(\n",
    "        geo_data=df_hot_geo,\n",
    "        name='choropleth',\n",
    "        data=df_hot_geo,\n",
    "        columns=['location_id', 'fluctuation_rank'],\n",
    "        key_on='feature.properties.location_id',\n",
    "        fill_color='RdYlBu',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=1.0,\n",
    "        threshold_scale=threshold_scale,\n",
    "        legend_name='fluctuation').add_to(m)\n",
    "    # always show zone id to the map as a label \n",
    "    folium.features.GeoJson(\n",
    "        df_hot_geo,\n",
    "        style_function=lambda feature: {\n",
    "            'fillColor': 'transparent',\n",
    "            'color': 'transparent',\n",
    "            'weight': 0,\n",
    "            'dashArray': '5, 5'\n",
    "        },\n",
    "        highlight_function=lambda x: {'weight':0.1, 'color':'black'},\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['location_id'],\n",
    "            aliases=['Zone ID:'],\n",
    "            localize=True,\n",
    "            sticky=False\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.LayerControl().add_to(m)\n",
    "    m.save(f'../data/figures/hot_zone_{year}.html')\n",
    "    # display(m)\n",
    "    return None\n",
    "\n",
    "\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2017)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2018)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2019)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2020)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经做了的：\n",
    "1. 给所有区域以年为单位进行流量和热度排名,其中：\n",
    "   1. 流量排名：以31天24小时流量和进行排名，这反映了其在这一个月总的流量\n",
    "   2. 热度排名：以31天24小时热度分数和进行排名，这反映了其在一个月中成为热点区域的频率\n",
    "2. 找到每年热点区域（用于后续时间序列预测）\n",
    "3. 流量排名和热度排名的差距显示了一些地区车流量的波动性显著强于另一些地区\n",
    "\n",
    "要做的：\n",
    "1. 比较各热点区域历年排名变化（可视化）\n",
    "2. 比较一天内热点变化（工作日，一般周末，圣诞假期）（可视化）\n",
    "3. auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间定性分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 比较历年热点区域变化情况\n",
    "2. 比较一天内热点变化趋势（工作日、周末、圣诞节）\n",
    "3. 比较同一区域的流量排名和热度排名，分类如下：\n",
    "   1. 流量排名/热度排名 > 1 ：时间波动性较强\n",
    "   2. 流量排名/热度排名 < 1 : 时间波动性较弱\n",
    "4. 分别可视化波动性较强和较弱的区域，观察其分布\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 确定用于ARIMA分析的10个区域\n",
    "2. 生成这些区域间交通流量的时间序列（5\\*10\\*10）（包括天气信息）\n",
    "3. 对500条时间序列进行ARIMA分解，获取其总体趋势和季节性趋势\n",
    "4. 总结结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>174840</th>\n",
       "      <td>236</td>\n",
       "      <td>73505.508875</td>\n",
       "      <td>77179.852388</td>\n",
       "      <td>70439.119330</td>\n",
       "      <td>104036.851335</td>\n",
       "      <td>76948.211696</td>\n",
       "      <td>402345.543625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175584</th>\n",
       "      <td>237</td>\n",
       "      <td>69581.175126</td>\n",
       "      <td>74142.578174</td>\n",
       "      <td>81220.208910</td>\n",
       "      <td>76967.548029</td>\n",
       "      <td>82742.029103</td>\n",
       "      <td>384890.539341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34968</th>\n",
       "      <td>48</td>\n",
       "      <td>48596.693008</td>\n",
       "      <td>58554.598364</td>\n",
       "      <td>52746.650200</td>\n",
       "      <td>49023.706104</td>\n",
       "      <td>61368.859444</td>\n",
       "      <td>270338.507120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119040</th>\n",
       "      <td>161</td>\n",
       "      <td>61327.012149</td>\n",
       "      <td>58747.474519</td>\n",
       "      <td>61489.543556</td>\n",
       "      <td>30068.336023</td>\n",
       "      <td>47698.763971</td>\n",
       "      <td>259492.130219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137640</th>\n",
       "      <td>186</td>\n",
       "      <td>41677.524535</td>\n",
       "      <td>43466.379761</td>\n",
       "      <td>45887.430686</td>\n",
       "      <td>47654.161498</td>\n",
       "      <td>29991.909295</td>\n",
       "      <td>208863.405776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170376</th>\n",
       "      <td>230</td>\n",
       "      <td>52021.850157</td>\n",
       "      <td>44907.335815</td>\n",
       "      <td>48159.681958</td>\n",
       "      <td>11367.842738</td>\n",
       "      <td>41913.865828</td>\n",
       "      <td>198600.576496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58032</th>\n",
       "      <td>79</td>\n",
       "      <td>52688.677901</td>\n",
       "      <td>46685.331204</td>\n",
       "      <td>42239.442257</td>\n",
       "      <td>19150.942597</td>\n",
       "      <td>35204.664157</td>\n",
       "      <td>196048.058117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104904</th>\n",
       "      <td>142</td>\n",
       "      <td>33905.663651</td>\n",
       "      <td>33778.761183</td>\n",
       "      <td>30174.380705</td>\n",
       "      <td>25960.416777</td>\n",
       "      <td>34668.904505</td>\n",
       "      <td>158630.126822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119784</th>\n",
       "      <td>162</td>\n",
       "      <td>33880.753928</td>\n",
       "      <td>33798.668072</td>\n",
       "      <td>32968.647014</td>\n",
       "      <td>17354.506946</td>\n",
       "      <td>24118.809119</td>\n",
       "      <td>142283.385078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97464</th>\n",
       "      <td>132</td>\n",
       "      <td>18065.873647</td>\n",
       "      <td>18137.291405</td>\n",
       "      <td>33368.103883</td>\n",
       "      <td>20755.444276</td>\n",
       "      <td>50773.052077</td>\n",
       "      <td>141231.765288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        location_id          2017          2018          2019           2020  \\\n",
       "174840          236  73505.508875  77179.852388  70439.119330  104036.851335   \n",
       "175584          237  69581.175126  74142.578174  81220.208910   76967.548029   \n",
       "34968            48  48596.693008  58554.598364  52746.650200   49023.706104   \n",
       "119040          161  61327.012149  58747.474519  61489.543556   30068.336023   \n",
       "137640          186  41677.524535  43466.379761  45887.430686   47654.161498   \n",
       "170376          230  52021.850157  44907.335815  48159.681958   11367.842738   \n",
       "58032            79  52688.677901  46685.331204  42239.442257   19150.942597   \n",
       "104904          142  33905.663651  33778.761183  30174.380705   25960.416777   \n",
       "119784          162  33880.753928  33798.668072  32968.647014   17354.506946   \n",
       "97464           132  18065.873647  18137.291405  33368.103883   20755.444276   \n",
       "\n",
       "                2021    total_score  \n",
       "174840  76948.211696  402345.543625  \n",
       "175584  82742.029103  384890.539341  \n",
       "34968   61368.859444  270338.507120  \n",
       "119040  47698.763971  259492.130219  \n",
       "137640  29991.909295  208863.405776  \n",
       "170376  41913.865828  198600.576496  \n",
       "58032   35204.664157  196048.058117  \n",
       "104904  34668.904505  158630.126822  \n",
       "119784  24118.809119  142283.385078  \n",
       "97464   50773.052077  141231.765288  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the 10 hostest zone in most years by adding 5 years' score together\n",
    "df_hot_5years = pd.DataFrame()\n",
    "df_hot_5years['location_id'] = hot_zones[2017]['location_id']\n",
    "for year in range(2017, 2022):\n",
    "    df_hot_5years[year] = hot_zones[year]['year_score']\n",
    "df_hot_5years['total_score'] = df_hot_5years.sum(axis=1)\n",
    "df_hot_5years = df_hot_5years.sort_values(by='total_score', ascending=False)\n",
    "# df_hot_5years = df_hot_5years.reset_index(drop=True)\n",
    "# df_hot_5years = nyc_zones_geo.merge(df_hot_5years, left_on='LocationID', right_on='location_id', how='right')\n",
    "# df_hot_5years = df_hot_5years[['location_id', 'total_score', 'geometry','zone']]\n",
    "\n",
    "## show the 10 hostest zone in most years\n",
    "# df_hot_5years[['location_id','zone', 'total_score']].head(10)\n",
    "df_hot_5years.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series for each pair of zones, and save them to a dictionary\n",
    "def create_time_series(year, location_id_list):\n",
    "    # create a template time series dataframe\n",
    "    count_df = pd.read_csv(f'../data/processed_nyc_data/countdata_{year}-12.csv')\n",
    "\n",
    "    temp_time_series = pd.DataFrame()\n",
    "    temp_time_series['date'] = np.repeat(np.arange(1, 32), 24)\n",
    "    temp_time_series['hour'] = np.tile(np.arange(0, 24), 31)\n",
    "    \n",
    "    all_time_series = {}\n",
    "    \n",
    "    for location_id1 in location_id_list:\n",
    "        for location_id2 in location_id_list:\n",
    "            if location_id1 != location_id2:\n",
    "                series_name = f'{location_id1}to{location_id2}'\n",
    "                all_time_series[series_name] = temp_time_series.copy()\n",
    "                temp_count = count_df.loc[(count_df['PULocationID']==location_id1) & (count_df['DOLocationID']==location_id2), :]\n",
    "                all_time_series[series_name] = all_time_series[series_name].merge(temp_count, left_on=['date', 'hour'], right_on=['day', 'hour'], how='left')\n",
    "                # fill the nan with 0\n",
    "                all_time_series[series_name] = all_time_series[series_name].fillna(0)\n",
    "                # keep the columns we need\n",
    "                all_time_series[series_name] = all_time_series[series_name][['date','hour','trip_count']]\n",
    "                # create a datetime column\n",
    "                all_time_series[series_name]['datetime']  = pd.to_datetime(all_time_series[series_name]['date'].astype(str) + '-12-' + str(year) + ' ' + all_time_series[series_name]['hour'].astype(str) + ':00:00', format='%d-%m-%Y %H:%M:%S')\n",
    "    return all_time_series\n",
    "\n",
    "\n",
    "\n",
    "location_id_list = df_hot_5years['location_id'].tolist()\n",
    "\n",
    "all_time_series_2017 = create_time_series(2017, location_id_list)\n",
    "all_time_series_2018 = create_time_series(2018, location_id_list)\n",
    "all_time_series_2019 = create_time_series(2019, location_id_list)\n",
    "all_time_series_2020 = create_time_series(2020, location_id_list)\n",
    "all_time_series_2021 = create_time_series(2021, location_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose the time series for each pair of zones\n",
    "def decompose_time_series(time_series_dict):\n",
    "    all_time_series_decomposed = {}\n",
    "    for time_series_key in time_series_dict.keys():\n",
    "        temp_series = time_series_dict[time_series_key]\n",
    "        all_time_series_decomposed[time_series_key] = seasonal_decompose(temp_series['trip_count'], model='additive', period=168).trend\n",
    "        time_series_dict[time_series_key] = temp_series\n",
    "    return all_time_series_decomposed\n",
    "\n",
    "#plot all time series in one figure\n",
    "def plot_all_time_series_decomposed(all_time_series, index, year, file_name):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    n = 0\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                # plot the time series without x axis label and y axis label\n",
    "                all_time_series[index[n]].plot(ax=ax[i, j], legend=False, xlabel=None, ylabel=None,xticks=[])\n",
    "                ax[i, j].set_xlabel(f'{index[n]}')\n",
    "                ax[i, j].set_title('')\n",
    "                n += 1\n",
    "    plt.savefig(f'../data/figures/{file_name}')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "all_time_series_2017_decomposed = decompose_time_series(all_time_series_2017)\n",
    "all_time_series_2018_decomposed = decompose_time_series(all_time_series_2018)\n",
    "all_time_series_2019_decomposed = decompose_time_series(all_time_series_2019)\n",
    "all_time_series_2020_decomposed = decompose_time_series(all_time_series_2020)\n",
    "all_time_series_2021_decomposed = decompose_time_series(all_time_series_2021)\n",
    "\n",
    "plot_all_time_series_decomposed(all_time_series_2017_decomposed, index, 2017,'time_series_2017_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2018_decomposed, index, 2018,'time_series_2018_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2019_decomposed, index, 2019,'time_series_2019_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2020_decomposed, index, 2020,'time_series_2020_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2021_decomposed, index, 2021,'time_series_2021_decomposed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get all index of the time series\n",
    "index = list(key for key in all_time_series_2017.keys())\n",
    "\n",
    "\n",
    "#plot all time series in one figure\n",
    "def plot_all_time_series(all_time_series, index, year, file_name):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    n = 0\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                # plot the time series without x axis label and y axis label\n",
    "                all_time_series[index[n]].plot(x='datetime', y='trip_count', ax=ax[i, j], legend=False, xlabel=None, ylabel=None,xticks=[])\n",
    "                ax[i, j].set_xlabel(f'{index[n]}')\n",
    "                ax[i, j].set_title('')\n",
    "                n += 1\n",
    "    plt.title(f'time series of taxi flow in {year}')\n",
    "    plt.savefig(f'../data/figures/{file_name}')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "plot_all_time_series(all_time_series_2017, index, 2017, 'time_series_2017.png')\n",
    "plot_all_time_series(all_time_series_2018, index, 2018, 'time_series_2018.png')\n",
    "plot_all_time_series(all_time_series_2019, index, 2019, 'time_series_2019.png')\n",
    "plot_all_time_series(all_time_series_2020, index, 2020, 'time_series_2020.png')\n",
    "plot_all_time_series(all_time_series_2021, index, 2021, 'time_series_2021.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get all index of the time series\n",
    "index = list(key for key in all_time_series_2021.keys())\n",
    "\n",
    "# time series arima model\n",
    "test_series = all_time_series_2021[index[81]]\n",
    "\n",
    "# add weather feature\n",
    "weather_2021 = pd.read_csv('../data/weather_data/csv/merged/weather_data_202112.csv')\n",
    "\n",
    "# add weather feature to time series\n",
    "test_series = test_series.merge(weather_2021, left_on=['date','hour'], right_on=['date','time'], how='left')\n",
    "test_series = test_series.drop(columns=['time','date','hour'])\n",
    "\n",
    "# set datetime as index\n",
    "test_series['datetime'] = pd.to_datetime(test_series['datetime'])\n",
    "test_series = test_series.set_index('datetime')\n",
    "\n",
    "# test_series.to_csv('../data/time_series_0.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pmdarima to find the best arima model\n",
    "stepwise_model = auto_arima(test_series['trip_count'], seasonal=True, m=24, \n",
    "                   exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']],\n",
    "                   suppress_warnings=True, \n",
    "                   error_action=\"ignore\", \n",
    "                   stepwise=True, \n",
    "                   max_order=None,\n",
    "                   trace=True,\n",
    "                   suppress_stdout=False,\n",
    "                   information_criterion='aic',)\n",
    "\n",
    "stepwise_model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pmdarima to find the best arima model\n",
    "stepwise_model1 = auto_arima(np.log(test_series['trip_count']+1), seasonal=True, m=24, \n",
    "                   exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']],\n",
    "                   suppress_warnings=True, \n",
    "                   error_action=\"ignore\", \n",
    "                   stepwise=True, \n",
    "                   max_order=None,\n",
    "                   trace=True,\n",
    "                   suppress_stdout=False,\n",
    "                   information_criterion='aic',)\n",
    "\n",
    "stepwise_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = '../data/model/arima_model_2019_132to236.sav'\n",
    "pickle.dump(stepwise_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = '../data/model/arima_model_2017_132to236.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_model1.fit(np.log(test_series['trip_count']+1),exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])\n",
    "print(result.summary())\n",
    "result.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = stepwise_model1.fit(np.log(test_series['trip_count']+1),exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])\n",
    "print(result1.summary())\n",
    "result1.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model1.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= stepwise_model.fit(test_series['trip_count'], exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = sarimax.SARIMAX(test_series['trip_count'], exog=test_series[['feels_like','humidity','wind_speed','rain','snow']], order=(3, 0, 0), seasonal_order=(2, 0, 0, 24))\n",
    "\n",
    "model1_fit = model1.fit()\n",
    "\n",
    "model1_fit.summary()\n",
    "\n",
    "model1_fit.plot_diagnostics(figsize=(15,12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model1 = stepwise_model\n",
    "result = stepwise_model1.fit(test_series['trip_count'])\n",
    "result.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a example of how to decompose a time series\n",
    "\n",
    "\n",
    "decomp = seasonal_decompose(test_series['trip_count'], model='additive',period=168)\n",
    "\n",
    "\n",
    "trend = decomp.trend\n",
    "seasonal = decomp.seasonal\n",
    "residual = decomp.resid\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(test_series['trip_count'], label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTUA_Ass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73aa197a937c6d4ce912cf3eb748cc19d4a43b8337a43d316b8eed91cf78bf92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
