{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Travel in New York City during the Christmas Holidays: An Analysis of Destinations, Trends, and Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from pmdarima import auto_arima\n",
    "from IPython.display import HTML\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace import sarimax\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集来源及获取\n",
    "本文主要用到了如下数据集：\n",
    "1. 纽约市2017年-2021每年12月的黄色出租车和绿色出租车数据 - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "2. 对应数据区域的shapefile - [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n",
    "3. 对应时间的天气数据（API获取） - [OpenWeather](https://openweathermap.org/api/one-call-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 天气数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入获取天气数据代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入区域地理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_zones_geo = gpd.read_file('../data/taxi_zones/taxi_zones.shp')\n",
    "#transform the coordinate system to WGS84\n",
    "nyc_zones_geo = nyc_zones_geo.to_crs(epsg=4326)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 出租车行程数据处理\n",
    "这一节旨在将出租车形成数据集计为小时车流数据，集计后的数据包括：\n",
    "1. 日期和小时\n",
    "2. 出发区域ID\n",
    "3. 到达区域ID\n",
    "4. 行程数量\n",
    "5. 行程人数\n",
    "6. 旅行费用\n",
    "7. 载客系数（行程人数/行程数量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 插入数据集计代码"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the hot points\n",
    "* calculate the in-and-out volumn for each zone by hours\n",
    "* give the zones a rank, transfer the rank to point\n",
    "* add all time points, find the hotest points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the score of each point per hour.   \n",
    "Because of the time complexity of bellow function is too high, I used AWS to calculate the traffic in each zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score for each zone by day and hours\n",
    "def get_in_out_volumn_score(df, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn.\n",
    "    Input:\n",
    "        df: a dataframe containing the counted data\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "    \"\"\"\n",
    "    # calculate the in-and-out volumn for each zone by day and hours\n",
    "    ## get the location id\n",
    "    location_id_list = list(range(1,264))\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## get the in-and-out volumn for each zone by day and hours, and store them in a dictionary\n",
    "    in_out_volumn = {}\n",
    "    for location_id in location_id_list:\n",
    "        for day in day_list:\n",
    "            for hour in hour_list:\n",
    "                in_out_volumn[(location_id, day, hour)] = [df.loc[(df['DOLocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum(), df.loc[(df['PULocationID']==location_id) & (df['day']==day) & (df['hour']==hour), 'trip_count'].sum()]\n",
    "        print(f'Finish calculating in-and-out volumn for zone {location_id} of {year}.')\n",
    "    # convert the dictionary to a dataframe\n",
    "    df_in_out_volumn = pd.DataFrame.from_dict(in_out_volumn, orient='index', columns=['in_volumn', 'out_volumn'])\n",
    "    # add day hour and location id as columns\n",
    "    df_in_out_volumn['day'] = df_in_out_volumn.index.map(lambda x: x[1])\n",
    "    df_in_out_volumn['hour'] = df_in_out_volumn.index.map(lambda x: x[2])\n",
    "    df_in_out_volumn['location_id'] = df_in_out_volumn.index.map(lambda x: x[0])\n",
    "    # add the total volumn\n",
    "    df_in_out_volumn['total_volumn'] = df_in_out_volumn['in_volumn'] + df_in_out_volumn['out_volumn']\n",
    "    # reorder the columns\n",
    "    df_in_out_volumn = df_in_out_volumn[['day', 'hour', 'location_id', 'in_volumn', 'out_volumn', 'total_volumn']]\n",
    "    # reset the index\n",
    "    df_in_out_volumn = df_in_out_volumn.reset_index(drop=True)\n",
    "\n",
    "    # calculate the rank of the total volumn for each zone by day and hours\n",
    "    df_in_out_volumn['total_volumn_rank'] = df_in_out_volumn.groupby(['day', 'hour'])['total_volumn'].rank(ascending=False)\n",
    "    # give the zone a score = 1/rank * number of zones\n",
    "    df_in_out_volumn['total_volumn_score'] = (1/df_in_out_volumn['total_volumn_rank']) * 263\n",
    "    \n",
    "    return df_in_out_volumn\n",
    "\n",
    "def plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year):\n",
    "    \"\"\"\n",
    "    Plot the score for each zone by day and hours.\n",
    "    Input:\n",
    "        df_in_out_volumn: a dataframe containing the in-and-out volumn for each zone by day and hours, and the score\n",
    "        day_list: a list of days\n",
    "        hour_list: a list of hours\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # plot the score for each zone by day and hours\n",
    "    ## get the day and hour\n",
    "    day_list = day_list\n",
    "    hour_list = hour_list\n",
    "    ## plot the score for each zone by day and hours\n",
    "    for day in day_list:\n",
    "        for hour in hour_list:\n",
    "            plt.figure(figsize=(20,10))\n",
    "            sns.barplot(x='location_id', y='total_volumn_score', data=df_in_out_volumn.loc[(df_in_out_volumn['day']==day) & (df_in_out_volumn['hour']==hour), :])\n",
    "            plt.title('day: {}, hour: {}'.format(day, hour))\n",
    "            plt.xticks([])\n",
    "            plt.savefig(f'../data/figures/in_out_volumn_score{year}_{day}_{hour}.png')\n",
    "            plt.close()\n",
    "\n",
    "def get_hot_score(year_list, day_list, hour_list):\n",
    "    for year in year_list:\n",
    "        # read the data\n",
    "        df = pd.read_csv(f'../data/processed_nyc_data/countdata_{year}-12.csv')\n",
    "        # calculate the in-and-out volumn for each zone by day and hours, and give each zone a score based on the volumn\n",
    "        df_in_out_volumn = get_in_out_volumn_score(df, day_list, hour_list,year)\n",
    "        df_in_out_volumn.to_csv(f'../data/processed_nyc_data/in_out_volumn_score_{year}-12.csv', index=False)\n",
    "        print(f'Finish getting score for {year}-12.')\n",
    "        # plot the score for each zone by day and hours\n",
    "        plot_in_out_volumn_score(df_in_out_volumn, day_list, hour_list, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # These codes run in AWS\n",
    "# year_list = range(2017, 2022)  # 2017-2021\n",
    "# day_list = range(1, 32)  # 1-31\n",
    "# hour_list = range(0, 24)  # 0-23\n",
    "# get_hot_score(year_list, day_list, hour_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find the hotest points for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(year):\n",
    "    df = pd.read_csv(f'../data/processed_nyc_data/{year}.csv')\n",
    "    df['year_score'] = df.groupby('location_id')['total_volumn_score'].transform('sum')\n",
    "    df['year_volumn'] = df.groupby('location_id')['total_volumn'].transform('sum')\n",
    "    df_hot = df[['location_id', 'year_score','year_volumn']].drop_duplicates()\n",
    "\n",
    "    df_hot['volumn_rank'] = df_hot['year_volumn'].rank(ascending=False)\n",
    "    df_hot['score_rank'] = df_hot['year_score'].rank(ascending=False)\n",
    "    \n",
    "    df_hot['fluctuation_coef'] = df_hot['score_rank'] / df_hot['volumn_rank']\n",
    "    df_hot['fluctuation_rank'] = df_hot['fluctuation_coef'].rank(ascending=False)\n",
    "    \n",
    "    df_hot = df_hot.sort_values(by='year_score', ascending=False)\n",
    "    df_hot = df_hot[['location_id', 'year_score', 'score_rank', 'year_volumn', 'volumn_rank', 'fluctuation_coef', 'fluctuation_rank']]\n",
    "\n",
    "    return df_hot\n",
    "\n",
    "def plot_hot_zone(df_hot, df_geo, year):\n",
    "    # merge the year score to the geodata\n",
    "    df_hot_geo = df_geo.merge(df_hot, left_on='LocationID', right_on='location_id', how='right')\n",
    "    plt.figure(figsize=(20,10))\n",
    "    df_hot_geo.plot(column='year_score', cmap='YlOrRd', edgecolor='black', legend=True)\n",
    "    plt.savefig(f'../data/figures/hot_zone_{year}.png')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "def get_zones_rank(year_list):\n",
    "    df_hots = {}\n",
    "    for year in year_list:\n",
    "        # get the 16 hotest zone for each year\n",
    "        df_hot = get_ranks(year)\n",
    "        df_hots[year] = df_hot\n",
    "\n",
    "    return df_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_zones = get_zones_rank(range(2017, 2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 30 hot zone's fluctuation rank on the map\n",
    "def plot_hot_zone(dict_hot, df_geo, year):\n",
    "    # merge the year score to the geodata\n",
    "    df_hot_geo = df_geo.merge(dict_hot[year], left_on='LocationID', right_on='location_id', how='right')\n",
    "\n",
    "    # filter the hot zone which volumn_rank is less than 31\n",
    "    df_hot_geo = df_hot_geo.loc[df_hot_geo['volumn_rank']<=30, :]\n",
    "\n",
    "    # add center point coordinates to the dataframe\n",
    "    df_hot_geo['center_lat'] = df_hot_geo['geometry'].centroid.y\n",
    "    df_hot_geo['center_lon'] = df_hot_geo['geometry'].centroid.x\n",
    "\n",
    "\n",
    "    threshold_scale = np.linspace(df_hot_geo['fluctuation_rank'].min(),\n",
    "                              df_hot_geo['fluctuation_rank'].max(), 4, dtype=float).tolist()\n",
    "\n",
    "    \n",
    "    # plot the hot zone with base map in folium\n",
    "    m = folium.Map(location=[40.75, -73.9], zoom_start=11, tiles='Stamen Toner')\n",
    "    folium.Choropleth(\n",
    "        geo_data=df_hot_geo,\n",
    "        name='choropleth',\n",
    "        data=df_hot_geo,\n",
    "        columns=['location_id', 'fluctuation_rank'],\n",
    "        key_on='feature.properties.location_id',\n",
    "        fill_color='RdYlBu',\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=1.0,\n",
    "        threshold_scale=threshold_scale,\n",
    "        legend_name='fluctuation').add_to(m)\n",
    "    # always show zone id to the map as a label \n",
    "    folium.features.GeoJson(\n",
    "        df_hot_geo,\n",
    "        style_function=lambda feature: {\n",
    "            'fillColor': 'transparent',\n",
    "            'color': 'transparent',\n",
    "            'weight': 0,\n",
    "            'dashArray': '5, 5'\n",
    "        },\n",
    "        highlight_function=lambda x: {'weight':0.1, 'color':'black'},\n",
    "        tooltip=folium.features.GeoJsonTooltip(\n",
    "            fields=['location_id'],\n",
    "            aliases=['Zone ID:'],\n",
    "            localize=True,\n",
    "            sticky=False\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    folium.LayerControl().add_to(m)\n",
    "    m.save(f'../data/figures/hot_zone_{year}.html')\n",
    "    # display(m)\n",
    "    return None\n",
    "\n",
    "\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2017)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2018)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2019)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2020)\n",
    "plot_hot_zone(hot_zones, nyc_zones_geo, 2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经做了的：\n",
    "1. 给所有区域以年为单位进行流量和热度排名,其中：\n",
    "   1. 流量排名：以31天24小时流量和进行排名，这反映了其在这一个月总的流量\n",
    "   2. 热度排名：以31天24小时热度分数和进行排名，这反映了其在一个月中成为热点区域的频率\n",
    "2. 找到每年热点区域（用于后续时间序列预测）\n",
    "3. 流量排名和热度排名的差距显示了一些地区车流量的波动性显著强于另一些地区\n",
    "\n",
    "要做的：\n",
    "1. 比较各热点区域历年排名变化（可视化）\n",
    "2. 比较一天内热点变化（工作日，一般周末，圣诞假期）（可视化）\n",
    "3. auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间定性分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 比较历年热点区域变化情况\n",
    "2. 比较一天内热点变化趋势（工作日、周末、圣诞节）\n",
    "3. 比较同一区域的流量排名和热度排名，分类如下：\n",
    "   1. 流量排名/热度排名 > 1 ：时间波动性较强\n",
    "   2. 流量排名/热度排名 < 1 : 时间波动性较弱\n",
    "4. 分别可视化波动性较强和较弱的区域，观察其分布\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "概览：\n",
    "1. 确定用于ARIMA分析的10个区域\n",
    "2. 生成这些区域间交通流量的时间序列（5\\*10\\*10）（包括天气信息）\n",
    "3. 对500条时间序列进行ARIMA分解，获取其总体趋势和季节性趋势\n",
    "4. 总结结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the 10 hostest zone in most years\n",
    "## add 5 years' score together\n",
    "df_hot_5years = pd.DataFrame()\n",
    "df_hot_5years['location_id'] = hot_zones[2017]['location_id']\n",
    "for year in range(2017, 2022):\n",
    "    df_hot_5years[year] = hot_zones[year]['year_score']\n",
    "df_hot_5years['total_score'] = df_hot_5years.sum(axis=1)\n",
    "df_hot_5years = df_hot_5years.sort_values(by='total_score', ascending=False)\n",
    "df_hot_5years = df_hot_5years.reset_index(drop=True)\n",
    "df_hot_5years = df_hot_5years.loc[:9, :]\n",
    "df_hot_5years = nyc_zones_geo.merge(df_hot_5years, left_on='LocationID', right_on='location_id', how='right')\n",
    "df_hot_5years = df_hot_5years[['location_id', 'total_score', 'geometry','zone']]\n",
    "\n",
    "## show a table of the 10 hostest zone in most years\n",
    "df_hot_5years[['zone', 'total_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series for each pair of zones, and save them to a dictionary\n",
    "def create_time_series(year, location_id_list):\n",
    "    # create a template time series dataframe\n",
    "    count_df = pd.read_csv(f'../data/processed_nyc_data/countdata_{year}-12.csv')\n",
    "\n",
    "    temp_time_series = pd.DataFrame()\n",
    "    temp_time_series['date'] = np.repeat(np.arange(1, 32), 24)\n",
    "    temp_time_series['hour'] = np.tile(np.arange(0, 24), 31)\n",
    "    \n",
    "    all_time_series = {}\n",
    "    \n",
    "    for location_id1 in location_id_list:\n",
    "        for location_id2 in location_id_list:\n",
    "            if location_id1 != location_id2:\n",
    "                series_name = f'{location_id1}to{location_id2}'\n",
    "                all_time_series[series_name] = temp_time_series.copy()\n",
    "                temp_count = count_df.loc[(count_df['PULocationID']==location_id1) & (count_df['DOLocationID']==location_id2), :]\n",
    "                all_time_series[series_name] = all_time_series[series_name].merge(temp_count, left_on=['date', 'hour'], right_on=['day', 'hour'], how='left')\n",
    "                # fill the nan with 0\n",
    "                all_time_series[series_name] = all_time_series[series_name].fillna(0)\n",
    "                # keep the columns we need\n",
    "                all_time_series[series_name] = all_time_series[series_name][['date','hour','trip_count']]\n",
    "                # create a datetime column\n",
    "                all_time_series[series_name]['datetime']  = pd.to_datetime(all_time_series[series_name]['date'].astype(str) + '-12-' + str(year) + ' ' + all_time_series[series_name]['hour'].astype(str) + ':00:00', format='%d-%m-%Y %H:%M:%S')\n",
    "    return all_time_series\n",
    "\n",
    "\n",
    "\n",
    "location_id_list = df_hot_5years['location_id'].tolist()\n",
    "\n",
    "all_time_series_2017 = create_time_series(2017, location_id_list)\n",
    "all_time_series_2018 = create_time_series(2018, location_id_list)\n",
    "all_time_series_2019 = create_time_series(2019, location_id_list)\n",
    "all_time_series_2020 = create_time_series(2020, location_id_list)\n",
    "all_time_series_2021 = create_time_series(2021, location_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose the time series for each pair of zones\n",
    "def decompose_time_series(time_series_dict):\n",
    "    all_time_series_decomposed = {}\n",
    "    for time_series_key in time_series_dict.keys():\n",
    "        temp_series = time_series_dict[time_series_key]\n",
    "        all_time_series_decomposed[time_series_key] = seasonal_decompose(temp_series['trip_count'], model='additive', period=168).trend\n",
    "        time_series_dict[time_series_key] = temp_series\n",
    "    return all_time_series_decomposed\n",
    "\n",
    "#plot all time series in one figure\n",
    "def plot_all_time_series_decomposed(all_time_series, index, year, file_name):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    n = 0\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                # plot the time series without x axis label and y axis label\n",
    "                all_time_series[index[n]].plot(ax=ax[i, j], legend=False, xlabel=None, ylabel=None,xticks=[])\n",
    "                ax[i, j].set_xlabel(f'{index[n]}')\n",
    "                ax[i, j].set_title('')\n",
    "                n += 1\n",
    "    plt.savefig(f'../data/figures/{file_name}')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "all_time_series_2017_decomposed = decompose_time_series(all_time_series_2017)\n",
    "all_time_series_2018_decomposed = decompose_time_series(all_time_series_2018)\n",
    "all_time_series_2019_decomposed = decompose_time_series(all_time_series_2019)\n",
    "all_time_series_2020_decomposed = decompose_time_series(all_time_series_2020)\n",
    "all_time_series_2021_decomposed = decompose_time_series(all_time_series_2021)\n",
    "\n",
    "plot_all_time_series_decomposed(all_time_series_2017_decomposed, index, 2017,'time_series_2017_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2018_decomposed, index, 2018,'time_series_2018_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2019_decomposed, index, 2019,'time_series_2019_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2020_decomposed, index, 2020,'time_series_2020_decomposed.png')\n",
    "plot_all_time_series_decomposed(all_time_series_2021_decomposed, index, 2021,'time_series_2021_decomposed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get all index of the time series\n",
    "index = list(key for key in all_time_series_2017.keys())\n",
    "\n",
    "\n",
    "#plot all time series in one figure\n",
    "def plot_all_time_series(all_time_series, index, year, file_name):\n",
    "    fig, ax = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    n = 0\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            if i != j:\n",
    "                # plot the time series without x axis label and y axis label\n",
    "                all_time_series[index[n]].plot(x='datetime', y='trip_count', ax=ax[i, j], legend=False, xlabel=None, ylabel=None,xticks=[])\n",
    "                ax[i, j].set_xlabel(f'{index[n]}')\n",
    "                ax[i, j].set_title('')\n",
    "                n += 1\n",
    "    plt.title(f'time series of taxi flow in {year}')\n",
    "    plt.savefig(f'../data/figures/{file_name}')\n",
    "    plt.close()\n",
    "    return None\n",
    "\n",
    "plot_all_time_series(all_time_series_2017, index, 2017, 'time_series_2017.png')\n",
    "plot_all_time_series(all_time_series_2018, index, 2018, 'time_series_2018.png')\n",
    "plot_all_time_series(all_time_series_2019, index, 2019, 'time_series_2019.png')\n",
    "plot_all_time_series(all_time_series_2020, index, 2020, 'time_series_2020.png')\n",
    "plot_all_time_series(all_time_series_2021, index, 2021, 'time_series_2021.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## get all index of the time series\n",
    "index = list(key for key in all_time_series_2021.keys())\n",
    "\n",
    "# time series arima model\n",
    "test_series = all_time_series_2021[index[81]]\n",
    "\n",
    "# add weather feature\n",
    "weather_2021 = pd.read_csv('../data/weather_data/csv/merged/weather_data_202112.csv')\n",
    "\n",
    "# add weather feature to time series\n",
    "test_series = test_series.merge(weather_2021, left_on=['date','hour'], right_on=['date','time'], how='left')\n",
    "test_series = test_series.drop(columns=['time','date','hour'])\n",
    "\n",
    "# set datetime as index\n",
    "test_series['datetime'] = pd.to_datetime(test_series['datetime'])\n",
    "test_series = test_series.set_index('datetime')\n",
    "\n",
    "# test_series.to_csv('../data/time_series_0.csv', index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pmdarima to find the best arima model\n",
    "stepwise_model = auto_arima(test_series['trip_count'], seasonal=True, m=24, \n",
    "                   exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']],\n",
    "                   suppress_warnings=True, \n",
    "                   error_action=\"ignore\", \n",
    "                   stepwise=True, \n",
    "                   max_order=None,\n",
    "                   trace=True,\n",
    "                   suppress_stdout=False,\n",
    "                   information_criterion='aic',)\n",
    "\n",
    "stepwise_model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pmdarima to find the best arima model\n",
    "stepwise_model1 = auto_arima(np.log(test_series['trip_count']+1), seasonal=True, m=24, \n",
    "                   exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']],\n",
    "                   suppress_warnings=True, \n",
    "                   error_action=\"ignore\", \n",
    "                   stepwise=True, \n",
    "                   max_order=None,\n",
    "                   trace=True,\n",
    "                   suppress_stdout=False,\n",
    "                   information_criterion='aic',)\n",
    "\n",
    "stepwise_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = '../data/model/arima_model_2019_132to236.sav'\n",
    "pickle.dump(stepwise_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "filename = '../data/model/arima_model_2017_132to236.sav'\n",
    "model = pickle.load(open(filename, 'rb'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_model1.fit(np.log(test_series['trip_count']+1),exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])\n",
    "print(result.summary())\n",
    "result.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = stepwise_model1.fit(np.log(test_series['trip_count']+1),exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])\n",
    "print(result1.summary())\n",
    "result1.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model1.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds= stepwise_model.fit(test_series['trip_count'], exogenous=test_series[['feels_like','humidity','wind_speed','rain','snow']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = sarimax.SARIMAX(test_series['trip_count'], exog=test_series[['feels_like','humidity','wind_speed','rain','snow']], order=(3, 0, 0), seasonal_order=(2, 0, 0, 24))\n",
    "\n",
    "model1_fit = model1.fit()\n",
    "\n",
    "model1_fit.summary()\n",
    "\n",
    "model1_fit.plot_diagnostics(figsize=(15,12))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_model1 = stepwise_model\n",
    "result = stepwise_model1.fit(test_series['trip_count'])\n",
    "result.plot_diagnostics(figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a example of how to decompose a time series\n",
    "\n",
    "\n",
    "decomp = seasonal_decompose(test_series['trip_count'], model='additive',period=168)\n",
    "\n",
    "\n",
    "trend = decomp.trend\n",
    "seasonal = decomp.seasonal\n",
    "residual = decomp.resid\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(test_series['trip_count'], label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal,label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTUA_Ass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73aa197a937c6d4ce912cf3eb748cc19d4a43b8337a43d316b8eed91cf78bf92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
